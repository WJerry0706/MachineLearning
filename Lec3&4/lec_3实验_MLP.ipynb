{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJsUtlaN-sW_"
      },
      "source": [
        "# <机器学习>课程 Lecture 3 实验"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g18xwaXu-sXB"
      },
      "source": [
        "## MLP 多层感知机\n",
        "\n",
        "给定一组数据,其输入维度为2,输出维度为1.\n",
        "请补全MLP的BP算法的计算过程,实现模型的分类."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAvskCyF-sXB"
      },
      "source": [
        "首先加载数据,并可视化,观察模型是否线性可分."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_ZXRDQI-sXB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data_filename = 'Lec2实验/cls_data_mlp.npy'\n",
        "cls_data = np.load(data_filename)\n",
        "x_data, y_data = cls_data[:, :-1], cls_data[:, -1]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data,\n",
        "    train_size=0.8, shuffle=True,\n",
        "    stratify=y_data\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "x_train_std = scaler.fit_transform(x_train)\n",
        "x_test_std = scaler.fit_transform(x_test)\n",
        "\n",
        "# x_data in [b, c_in]\n",
        "c_in = x_data.shape[1]\n",
        "# y_data in [b, c_out]\n",
        "c_out = 1\n",
        "#test file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-QEyks5-sXC"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "pos_data = x_data[y_data == 1, :]\n",
        "neg_data = x_data[y_data == 0, :]\n",
        "\n",
        "plt.scatter(pos_data[:, 0], pos_data[:, 1], c='red')\n",
        "plt.scatter(neg_data[:, 0], neg_data[:, 1], c='blue')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPmb9SPW-sXD"
      },
      "source": [
        "首先定义若干激活函数,包括sigmoid函数和ReLU函数.\n",
        "\n",
        "同时定义一个预测准确率的辅助函数."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gh2cgeS-sXD"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def sigmoid(val):\n",
        "    \"\"\" sigmoid function \"\"\"\n",
        "    return np.exp(val) / ( 1. + np.exp(val))\n",
        "\n",
        "def relu(val):\n",
        "    \"\"\" relu activation \"\"\"\n",
        "    return np.where(val >= 0, val, 0)\n",
        "\n",
        "# 考虑实现更多的激活函数,例如 tanh, mish, swish等\n",
        "def _get_activation(activation:str='relu'):\n",
        "    if activation == 'relu':\n",
        "        return relu\n",
        "    elif activation == 'sigmoid':\n",
        "        return sigmoid\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"activation {activation} is not supported\"\n",
        "        )\n",
        "\n",
        "def calc_accu(pred: List[int], target: List[int]) -> float:\n",
        "    \"\"\" calculate classification accuracy \"\"\"\n",
        "    assert pred.shape[0] == target.shape[0], \\\n",
        "            f\"inputs and labels should be in same shape but \\\n",
        "              get inputs in {pred.shape} and lables in {target.shape}\"\n",
        "    assert np.min(pred) > 0 and np.max(pred) < 1.0, \\\n",
        "            f\"inputs should between 0 and 1, \\\n",
        "              but get {np.min(pred)} - {np.max(pred)}\"\n",
        "    if len(pred.shape) == 2:\n",
        "        pred = pred[:, 0]\n",
        "    assert pred.shape == target.shape\n",
        "\n",
        "    return ((pred > 0.5) == target).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLEm1v17-sXD"
      },
      "source": [
        "针对二分类,我们使用NLL损失,即使用模型预测的概率计算负对数似然.\n",
        "\n",
        "计算公式\n",
        "$$\\mathcal L = -\\sum y\\log p$$\n",
        "\n",
        "计算损失函数到输入值的梯度为\n",
        "$$\\frac{\\partial \\mathcal L}{\\partial p} = - \\sum y \\frac{1}{p}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgGRNbMF-sXD"
      },
      "outputs": [],
      "source": [
        "# 定义损失函数及其梯度\n",
        "class NLLLoss(object):\n",
        "    \"\"\" Negative Log Likelihood Loss\"\"\"\n",
        "    def __init__(self, reduction:str=None):\n",
        "        self.reduction = reduction\n",
        "\n",
        "        # 保留上下文用于计算梯度\n",
        "        self.ctx_inputs = None\n",
        "        self.ctx_labels = None\n",
        "\n",
        "    def forward(self, inputs, labels):\n",
        "        \"\"\" calculate loss \"\"\"\n",
        "        assert inputs.shape[0] == labels.shape[0], \\\n",
        "            f\"inputs and labels should be in same number of samples \\\n",
        "              but get {inputs.shape[0]} inputs and {labels.shape[0]} lables\"\n",
        "        assert np.min(inputs) > 0 and np.max(inputs) < 1.0, \\\n",
        "            f\"inputs should between 0 and 1, \\\n",
        "                but get value from {np.min(inputs):.4f} to {np.max(inputs):.4f}\"\n",
        "\n",
        "        if len(inputs.shape) == 2:\n",
        "            labels = labels[:, None]\n",
        "        assert inputs.shape == labels.shape\n",
        "\n",
        "        # 计算损失\n",
        "        likelihood = np.multiply(labels, np.log(inputs)) \\\n",
        "             + np.multiply((1 - labels), np.log(1 - inputs))\n",
        "        loss = - likelihood\n",
        "\n",
        "        # 保存上下文信息\n",
        "        self.ctx_inputs = inputs\n",
        "        self.ctx_labels = labels\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return np.mean(loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            return np.sum(loss)\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        \"\"\" calculate gradient \"\"\"\n",
        "\n",
        "        # === 请补全NLL损失的梯度计算 ===\n",
        "        grad = None\n",
        "\n",
        "        return  None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMLM_dFR-sXD"
      },
      "source": [
        "对于mlp中的每一层,定义一层感知器.\n",
        "包括正向传播,反向传播和梯度更新.\n",
        "\n",
        "其中正向传播公式为\n",
        "$$y = \\sigma(z) = \\sigma(w^Tx + b)$$\n",
        "其中z为输入特征的加权和,保存在ctx_hidden变量中用于计算残差\n",
        "\n",
        "反向传播过程中,首先计算残差\n",
        "$$\\frac{\\partial \\mathcal L}{\\partial z} = \\frac{\\partial \\mathcal L}{\\partial y} \\frac{\\partial y}{\\partial z}$$\n",
        "\n",
        "可以得到损失函数对权重的梯度,保存在上下文中用于梯度更新.\n",
        "同时需要返回损失函数对前一层输出的梯度,以便前一层的梯度计算."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sod3CU8u-sXE"
      },
      "outputs": [],
      "source": [
        "class LinearLayer:\n",
        "    \"\"\" Perceptron Layer in MLP \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        c_in: int, c_out: int,\n",
        "        init_mean: float, init_var: float,\n",
        "        bias:bool = False,\n",
        "        activation: str = 'relu'\n",
        "    ) -> None:\n",
        "\n",
        "        self.bias = bias\n",
        "        if self.bias:\n",
        "            weight_size = (c_in+1, c_out)\n",
        "        else:\n",
        "            weight_size = (c_in, c_out)\n",
        "\n",
        "        self.grad = None\n",
        "        self.weight = np.random.normal(\n",
        "            init_mean, init_var,\n",
        "            size=weight_size\n",
        "        )\n",
        "\n",
        "        self.activation = _get_activation(activation)\n",
        "\n",
        "        self.ctx_inputs = None\n",
        "        self.ctx_hidden = None\n",
        "        self.ctx_outputs = None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" forward the network \"\"\"\n",
        "        bsz = inputs.shape[0]\n",
        "\n",
        "        if self.bias:\n",
        "            self.ctx_inputs = np.concatenate([\n",
        "                inputs,\n",
        "                np.ones(shape=(bsz, 1))\n",
        "            ], axis=1)\n",
        "        else:\n",
        "            self.ctx_inputs = inputs\n",
        "\n",
        "        self.ctx_hidden = np.matmul(self.ctx_inputs, self.weight)\n",
        "        self.ctx_outputs = self.activation(self.ctx_hidden)\n",
        "\n",
        "        return self.ctx_outputs\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        \"\"\" calculate the gradient \"\"\"\n",
        "        assert self.ctx_hidden.shape == prev_grad.shape, \\\n",
        "            f\"expected same shape of ctx_hidden and prev_grad, \\\n",
        "              but get ctx_hidden in {self.ctx_hidden.shape} and \\\n",
        "              prev_grad in {prev_grad.shape}\"\n",
        "\n",
        "        # === 请补全三种激活函数的梯度计算 ===\n",
        "        if self.activation.__name__ == 'relu':\n",
        "            residual = None\n",
        "        elif self.activation.__name__ == 'sigmoid':\n",
        "            residual = None\n",
        "        else:\n",
        "            raise NotImplementedError(f\"activation not supported\")\n",
        "\n",
        "        # === 请补全单层感知机中参数的梯度计算 ===\n",
        "        # === 可以参考上一次实验课的代码 ===\n",
        "        self.grad = None\n",
        "\n",
        "        # === 请补全单层感知机对输入的梯度计算 ===\n",
        "        # === 即损失函数对前一层的输出的梯度 ===\n",
        "        if self.bias:\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\" update weight with the gradient \"\"\"\n",
        "        assert self.grad is not None\n",
        "\n",
        "        self.weight = self.weight - learning_rate * self.grad\n",
        "        self.grad = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmfTqHIK-sXE"
      },
      "source": [
        "最后定义完整的MLP模型,包括多个感知器层.\n",
        "\n",
        "默认情况下浅层模型中间层使用sigmoid激活,最后一层使用sigmoid激活函数(分类任务).\n",
        "\n",
        "前向传播的时候逐层传播,然后计算损失值.\n",
        "\n",
        "从损失值开始逐层反向传播,计算每一层权重的梯度.\n",
        "\n",
        "最后更新每一层的权重"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7rD4WiO-sXE"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\" Multi Layer Peceptron \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_feat_list,\n",
        "        init_mean,\n",
        "        init_val,\n",
        "    ) -> None:\n",
        "        assert isinstance(num_feat_list, list), \\\n",
        "            f\"expected num_feat_list is a list of int \\\n",
        "                but get a {type(num_feat_list)}\"\n",
        "        assert isinstance(num_feat_list[0], int), \\\n",
        "            f\"expected num_feat_list is a list of int \\\n",
        "                but get a list of {type(num_feat_list[0])}\"\n",
        "\n",
        "        self.num_layers = len(num_feat_list) - 1\n",
        "\n",
        "        self.layers = []\n",
        "        for i in range(1, 1 + self.num_layers):\n",
        "            if i != self.num_layers:\n",
        "                self.layers.append(\n",
        "                    LinearLayer(num_feat_list[i-1], num_feat_list[i],\n",
        "                                init_mean=init_mean, init_var=init_val,\n",
        "                                bias=False, activation='sigmoid')\n",
        "                )\n",
        "            else:\n",
        "                self.layers.append(\n",
        "                    LinearLayer(num_feat_list[i-1], num_feat_list[i],\n",
        "                                init_mean=init_mean, init_var=init_val,\n",
        "                                bias=True, activation='sigmoid')\n",
        "                )\n",
        "\n",
        "        self.loss_fn = NLLLoss(reduction='mean')\n",
        "\n",
        "        print(\"initialize a mlp model:\")\n",
        "        print(\"layer \\t weight.shape \\t activation\")\n",
        "        for idx, item in enumerate(self.layers):\n",
        "            print(f\"{idx}\\t{item.weight.shape}\\t{item.activation.__name__}\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ''' forward layer by layer '''\n",
        "        res = inputs\n",
        "        for layer in self.layers:\n",
        "            res = layer.forward(res)\n",
        "\n",
        "        return res\n",
        "\n",
        "    def fit(self,\n",
        "        inputs: np.ndarray,\n",
        "        labels: np.ndarray,\n",
        "        lr: float,\n",
        "        epochs:int\n",
        "    )->None:\n",
        "        \"\"\" fit mlp model with gradient descent \"\"\"\n",
        "\n",
        "        for epoch_idx in range(epochs):\n",
        "            # === forward and loss ===\n",
        "            res = self.forward(inputs)\n",
        "            loss = self.loss_fn.forward(res, labels)\n",
        "\n",
        "            # === backward ===\n",
        "            # === 请补全反向传播的过程 ===\n",
        "            # === 即对于每一层依次反向利用链式法则计算梯度 ===\n",
        "            pass\n",
        "\n",
        "            # === update ===\n",
        "            for layer in self.layers:\n",
        "                layer.update(lr)\n",
        "\n",
        "            pred_train = self.forward(inputs)\n",
        "            accu = calc_accu(pred_train, labels)\n",
        "\n",
        "            print(f\"{epoch_idx:03d}/{epochs}: loss = {loss:.4f}, accu = {accu.mean():.2%}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdsGsCNR-sXE"
      },
      "outputs": [],
      "source": [
        "# 考虑修改不同的模型结构,例如[2, 16, 1]或者[2, 4, 4, 1]等\n",
        "# 考虑修改不同的初始化参数, 可以自行查阅资料调研xavier_init或者kaiming_init等\n",
        "model = MLP([2, 8, 1], init_mean=0, init_val=0.1)\n",
        "\n",
        "x_test_pred = model.forward(x_test)\n",
        "print('before train: ', calc_accu(x_test_pred, y_test))\n",
        "\n",
        "model.fit(x_train, y_train, lr=0.001, epochs=100)\n",
        "\n",
        "x_test_pred = model.forward(x_test)\n",
        "print('after train: ', calc_accu(x_test_pred, y_test))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}